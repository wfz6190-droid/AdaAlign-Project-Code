"""
Text Encoder with PEFT support for SBIR
Supports CLIP and SiGLIP models with Adapter/LoRA
"""
import os
import torch
import torch.nn as nn
from transformers import CLIPTextModel, SiglipTextModel, AutoTokenizer
from modeling.adapter import AdapterBottleneck

# è‡ªåŠ¨é…ç½®HuggingFaceé•œåƒå’Œä»£ç†è®¾ç½®ï¼ˆå›½å†…ç½‘ç»œä¼˜åŒ–ï¼‰
def _configure_huggingface_environment():
    """é…ç½®HuggingFaceç¯å¢ƒï¼Œç¦ç”¨ä»£ç†å¹¶ä½¿ç”¨é•œåƒ"""
    import os
    
    # 1. æ¸…é™¤æ‰€æœ‰ä»£ç†ç¯å¢ƒå˜é‡
    proxy_vars = ['http_proxy', 'https_proxy', 'HTTP_PROXY', 'HTTPS_PROXY', 
                  'all_proxy', 'ALL_PROXY', 'no_proxy', 'NO_PROXY',
                  'ftp_proxy', 'FTP_PROXY', 'rsync_proxy', 'RSYNC_PROXY']
    
    cleared = []
    for var in proxy_vars:
        if var in os.environ:
            cleared.append(var)
            del os.environ[var]
    
    if cleared:
        print(f"âš ï¸  å·²æ¸…é™¤ {len(cleared)} ä¸ªä»£ç†è®¾ç½®: {', '.join(cleared)}")
    
    # 2. è®¾ç½®HuggingFaceé•œåƒ
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    os.environ['TRANSFORMERS_OFFLINE'] = '0'  # ç¡®ä¿ä¸æ˜¯ç¦»çº¿æ¨¡å¼
    
    # 3. ç¦ç”¨requestsçš„ç¯å¢ƒä»£ç†
    try:
        import requests
        # å¼ºåˆ¶requestsä¸ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†
        original_session = requests.Session
        
        def no_proxy_session():
            session = original_session()
            session.trust_env = False  # å…³é”®ï¼šä¸ä¿¡ä»»ç¯å¢ƒå˜é‡
            return session
        
        requests.Session = no_proxy_session
        requests.sessions.Session = no_proxy_session
        print("âœ“ å·²ç¦ç”¨requestsä»£ç†")
    except:
        pass
    
    print(f"âœ“ HuggingFaceé•œåƒ: {os.environ.get('HF_ENDPOINT')}")
    
# æ‰§è¡Œé…ç½®
_configure_huggingface_environment()


class TextEncoderWithAdapter(nn.Module):
    """Text Encoder wrapper with Adapter support"""
    def __init__(self, model_name='openai/clip-vit-base-patch32', add_adapter=True, adapter_reduction=4, freeze_base=True, output_dim=1024):
        super(TextEncoderWithAdapter, self).__init__()
        self.model_name = model_name
        self.add_adapter = add_adapter
        self.output_dim = output_dim  # ç›®æ ‡è¾“å‡ºç»´åº¦ï¼ˆä¸è§†è§‰ç‰¹å¾åŒ¹é…ï¼‰
        
        print(f"Loading text encoder: {model_name}...")
        
        # Load text encoder based on model type
        try:
            if 'clip' in model_name.lower():
                print(f"  æ­£åœ¨ä» {os.environ.get('HF_ENDPOINT', 'huggingface.co')} åŠ è½½CLIPæ¨¡å‹...")
                print(f"  æ¨¡å‹: {model_name}")
                self.text_encoder = CLIPTextModel.from_pretrained(
                    model_name,
                    resume_download=True,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
                    local_files_only=False  # å…è®¸ä¸‹è½½
                )
                self.hidden_size = self.text_encoder.config.hidden_size
                print(f"  âœ“ CLIPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œéšè—å±‚ç»´åº¦: {self.hidden_size}")
            elif 'siglip' in model_name.lower():
                print(f"  æ­£åœ¨ä» {os.environ.get('HF_ENDPOINT', 'huggingface.co')} åŠ è½½SiGLIPæ¨¡å‹...")
                print(f"  æ¨¡å‹: {model_name}")
                from transformers import SiglipModel
                siglip_model = SiglipModel.from_pretrained(
                    model_name,
                    resume_download=True,
                    local_files_only=False
                )
                self.text_encoder = siglip_model.text_model
                self.hidden_size = self.text_encoder.config.hidden_size
                print(f"  âœ“ SiGLIPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œéšè—å±‚ç»´åº¦: {self.hidden_size}")
            else:
                raise ValueError(f"Unsupported model: {model_name}")
        except Exception as e:
            print(f"\nâŒ åŠ è½½æ–‡æœ¬ç¼–ç å™¨å¤±è´¥: {e}")
            print(f"\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print(f"  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"  2. ä½¿ç”¨é•œåƒæº: export HF_ENDPOINT=https://hf-mirror.com")
            print(f"  3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°: https://hf-mirror.com/{model_name}")
            print(f"  4. å¦‚æœå·²ä¸‹è½½ï¼Œè®¾ç½®ç¼“å­˜è·¯å¾„: export HF_HOME=/path/to/cache")
            raise
        
        # Add projection layer if output_dim differs from hidden_size
        if self.hidden_size != self.output_dim:
            self.projection = nn.Linear(self.hidden_size, self.output_dim, bias=False)
            print(f"  âœ“ æ·»åŠ æŠ•å½±å±‚: {self.hidden_size} -> {self.output_dim}")
        else:
            self.projection = None
        
        # Freeze base model if specified
        if freeze_base:
            for param in self.text_encoder.parameters():
                param.requires_grad = False
        
        # Add adapters to transformer layers
        if self.add_adapter:
            self._inject_adapters(adapter_reduction)
    
    def _inject_adapters(self, reduction=4):
        """Inject adapter modules after each transformer layer"""
        # For CLIP/SiGLIP, adapters are added after MLP in each layer
        # CLIP uses text_model.encoder.layers, SiGLIP uses encoder.layers
        if hasattr(self.text_encoder, 'text_model'):
            encoder_layers = self.text_encoder.text_model.encoder.layers  # CLIP
        elif hasattr(self.text_encoder, 'encoder'):
            encoder_layers = self.text_encoder.encoder.layers  # SiGLIP
        else:
            raise AttributeError("Cannot find encoder layers in text model")
        for layer_idx, layer in enumerate(encoder_layers):
            # Add adapter after MLP
            layer.adapter = AdapterBottleneck(self.hidden_size, reduction=reduction)
            
            # Modify forward to include adapter
            original_forward = layer.forward
            
            def make_forward_with_adapter(original_fwd, adapter_module):
                def forward_with_adapter(*args, **kwargs):
                    outputs = original_fwd(*args, **kwargs)
                    if isinstance(outputs, tuple):
                        hidden_states = outputs[0]
                        hidden_states = adapter_module(hidden_states)
                        return (hidden_states,) + outputs[1:]
                    else:
                        return adapter_module(outputs)
                return forward_with_adapter
            
            layer.forward = make_forward_with_adapter(original_forward, layer.adapter)
        
        print(f"âœ“ Injected adapters into {len(encoder_layers)} transformer layers")
    
    def forward(self, input_ids, attention_mask=None):
        """Forward pass through text encoder"""
        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        # Return pooled output (CLS token representation)
        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
            embeddings = outputs.pooler_output
        else:
            # For models without pooler, use last hidden state's first token
            embeddings = outputs.last_hidden_state[:, 0, :]
        
        # Apply projection if needed
        if self.projection is not None:
            embeddings = self.projection(embeddings)
        
        return embeddings
    
    def encode_text(self, texts, tokenizer, device='cuda'):
        """Encode text prompts into embeddings"""
        if isinstance(texts, str):
            texts = [texts]
        
        # Tokenize
        encoded = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
        input_ids = encoded['input_ids'].to(device)
        attention_mask = encoded['attention_mask'].to(device)
        
        # Get embeddings
        with torch.set_grad_enabled(self.training):
            embeddings = self.forward(input_ids, attention_mask)
        
        return embeddings


class TextEncoderWithLoRA(nn.Module):
    """Text Encoder wrapper with LoRA support"""
    def __init__(self, model_name='openai/clip-vit-base-patch32', lora_r=16, lora_alpha=16, 
                 lora_dropout=0.1, freeze_base=True, output_dim=1024):
        super(TextEncoderWithLoRA, self).__init__()
        from peft import LoraConfig, get_peft_model
        self.output_dim = output_dim
        
        # Load base model
        if 'clip' in model_name.lower():
            base_model = CLIPTextModel.from_pretrained(model_name)
            self.hidden_size = base_model.config.hidden_size
        elif 'siglip' in model_name.lower():
            from transformers import SiglipModel
            siglip_model = SiglipModel.from_pretrained(model_name)
            base_model = siglip_model.text_model
            self.hidden_size = base_model.config.hidden_size
        else:
            raise ValueError(f"Unsupported model: {model_name}")
        
        # Configure LoRA
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=["q_proj", "v_proj"],  # Apply to attention Q and V projections
            lora_dropout=lora_dropout,
            bias="none",
            task_type="FEATURE_EXTRACTION"
        )
        
        # Apply LoRA
        self.text_encoder = get_peft_model(base_model, lora_config)
        self.text_encoder.print_trainable_parameters()
        
        # Add projection layer if needed
        if self.hidden_size != self.output_dim:
            self.projection = nn.Linear(self.hidden_size, self.output_dim, bias=False)
            print(f"  âœ“ æ·»åŠ æŠ•å½±å±‚: {self.hidden_size} -> {self.output_dim}")
        else:
            self.projection = None
    
    def forward(self, input_ids, attention_mask=None):
        """Forward pass through text encoder"""
        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
            embeddings = outputs.pooler_output
        else:
            embeddings = outputs.last_hidden_state[:, 0, :]
        
        # Apply projection if needed
        if self.projection is not None:
            embeddings = self.projection(embeddings)
        
        return embeddings
    
    def encode_text(self, texts, tokenizer, device='cuda'):
        """Encode text prompts into embeddings"""
        if isinstance(texts, str):
            texts = [texts]
        
        encoded = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
        input_ids = encoded['input_ids'].to(device)
        attention_mask = encoded['attention_mask'].to(device)
        
        with torch.set_grad_enabled(self.training):
            embeddings = self.forward(input_ids, attention_mask)
        
        return embeddings


def build_text_encoder(args):
    """Factory function to build text encoder with PEFT"""
    # Map arch_CLIP to HuggingFace model names
    # ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„CLIP Largeæ¨¡å‹ï¼ˆå·²å­˜åœ¨äºç³»ç»Ÿä¸­ï¼‰
    LOCAL_CACHED_MODEL = '/home/gpu/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/32bd64288804d66eefd0ccbe215aa642df71cc41'
    
    model_map = {
        'RN50': LOCAL_CACHED_MODEL,  # ä½¿ç”¨å·²ç¼“å­˜çš„CLIP Large
        'ViT-B/32': LOCAL_CACHED_MODEL,
        'ViT-B/16': LOCAL_CACHED_MODEL,
        'ViT-L/14': LOCAL_CACHED_MODEL,
        'siglip-base-p16': 'google/siglip-base-patch16-224',
    }
    
    model_name = model_map.get(args.arch_CLIP, 'openai/clip-vit-base-patch32')  # é»˜è®¤ä½¿ç”¨CLIP
    print(f"Text encoder model: {model_name} (from arch_CLIP={args.arch_CLIP})")
    
    # è®¾ç½®è¾“å‡ºç»´åº¦ï¼ˆä¸è§†è§‰ç‰¹å¾ç»´åº¦åŒ¹é…ï¼‰
    output_dim = getattr(args, 'clip_feature', 1024)  # é»˜è®¤1024ç»´
    
    if args.text_lora:
        text_encoder = TextEncoderWithLoRA(
            model_name=model_name,
            lora_r=args.text_lora_r,
            lora_alpha=args.text_lora_alpha,
            lora_dropout=args.text_lora_dropout,
            freeze_base=True,
            output_dim=output_dim
        )
    elif args.text_adapter:
        text_encoder = TextEncoderWithAdapter(
            model_name=model_name,
            add_adapter=True,
            adapter_reduction=args.text_adapter_reduction,
            freeze_base=True,
            output_dim=output_dim
        )
    else:
        # Return None if no PEFT is applied to text encoder
        return None
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    return text_encoder, tokenizer

